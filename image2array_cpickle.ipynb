{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and generate train, valid, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code to download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified ./notMNIST_large.tar.gz\n",
      "Found and verified ./notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '.' # Change me to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    #may download again by setting force=True\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code to extract the dataset from .tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./notMNIST_large already present - Skipping extraction of ./notMNIST_large.tar.gz.\n",
      "['./notMNIST_large/A', './notMNIST_large/B', './notMNIST_large/C', './notMNIST_large/D', './notMNIST_large/E', './notMNIST_large/F', './notMNIST_large/G', './notMNIST_large/H', './notMNIST_large/I', './notMNIST_large/J']\n",
      "./notMNIST_small already present - Skipping extraction of ./notMNIST_small.tar.gz.\n",
      "['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D', './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', './notMNIST_small/I', './notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACRUlEQVR4nG2STUiUYRDH//O8z368vvu9qGyZH7upaaYb9GEYlVFRknTJsFsWHTt1rEvnunSIQKhDF6EuBR6MwkzE0CQJKiFF11owd9VdM0133/eZDtruYs1pmP/8/swwA+TCZRB0GyhfKUi9zpbwSNn7KWJsD4Ezj4d4+tvtCvEfreH+GGcsjt/Nu+XbxO9qlmB/Sd0/qA0XlVLMnJ6/k5tkq0tkz3fNKwLYa2gV22wJh9itWRYYc5fLcsiW6HLWGUzmIni35yC4UCQzsj+saO3N+DipYNVplV+f4K58Ns4WD/uv9/1kjt2r3YQ2yeLETNQkNbA8s6wBpd7yvC1rqQtXVyRNJAID/SMxOE85vIpypLzkM3hpcHhBvf5RzDBaG3IkWc3NmkBqJQ271S9ZFe+Ih7S/trvML0BqcvirWF9K9whYnS1rNgASIOG5doRJD54ITB2eMU4u+bTMlbFpABLE/r1NToKokY3RwY6AYSfQgcjiIjGgaU3dSVaKmVmtWhvMzCY/DUtAgOjY2Q0wccYC67BDKQD1uhMQZLoDJT4I0yROILH6fZ0FC+j1RSDJjrZmMjj7cK79ecyfOqr37uuotVxVNz4kCLL+5ifO8pNzERR54LOHg/Zbr2bZ+tgDErqjuoZlKrYQx/qKXM5OpzIPRn0QO5NdmnA3ttpUenB2coMUm8zEWvrlrwX2tFWawt8ZRppGe9XWhZkVJoYcq7aIuUfi7bvS4y8eJQu/zdeth/qin+OQoUqbSycqFPWgLDec7e4/wqfzL3yRo74AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='notMNIST_small/A/MDEtMDEtMDAudHRm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert entire dataset into a 3D array (image_index,28,28) to store pixel_depth; normalize to approximately zero mean and ~0.5 standard deviation; pickle and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./notMNIST_large/A.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/B.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/C.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/D.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/E.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/F.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/G.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/H.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/I.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/J.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/A.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/B.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/C.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/D.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/E.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/F.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/G.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/H.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/I.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/J.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (imageio.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except (IOError, ValueError) as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fad40b9f9e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fad40e19ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=pickle.load(open(\"notMNIST_small/J.pickle\",\"rb\"))\n",
    "plt.imshow(data[0],cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge pickle files and generate train, validation, and test datasets with labels 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):      # label 0-9 \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter   # 3D arrary slice\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## randomize the data. It's important to have the labels well shuffled for the training and test distributions to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0]) #trick to make labels and datasets match\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the data for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 690800506\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
